{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 'Stan' for statistical inference\n",
    "\n",
    "Stan is a \"probabilistic programming language\" used to perform full Bayesian statistical inference.  In many settings, especially natural language processing contexts, this type of full Bayesian inference is hard to implement and computationally intensive.\n",
    "\n",
    "Stan offers a solution that is relatively easy to use and that runs efficiently.  Stan is written in C++ for performance but can be accessed through various interfaces (Python, R, Matlab, etc).  Today, I will go through a classification example using the Python interface, PyStan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document classification example\n",
    "\n",
    "This example is adapted from Section 14.3 of the Stan reference manual available [here](https://github.com/stan-dev/stan/releases/download/v2.9.0/stan-reference-2.9.0.pdf) and the Stan examples github available [here](https://github.com/stan-dev/example-models/wiki).\n",
    "\n",
    "We want to classify the topic of a document given its words.  We also have a collection of topic-labeled documents and their words to train on.  Let's use a Naive Bayes classifier.  Then given a new document, we need to calculate the posterior probabilities:\n",
    "\n",
    "$$ p( topic | words ) = \\frac{p(topic) \\times p(words | topic)}{p(words)} $$\n",
    "\n",
    "and then pick the topic which maximizes this posterior.  To do these calculations, we need to first infer $p(topic)$ and $p(words|topic)$.\n",
    "\n",
    "Let's flesh this model out further.  Assume the generative process is as follows.  Fix the number of topics to be $K$.  Our training data consists of $M$ documents made up of a bag of words drawn from a vocabulary of $V$ distinct words.  A document $m$ has $N_{m}$ words indexed as $w_{m,1},...,w_{m,N_{m}}$.  To keep this model simple, we will assume word order is not relevant.\n",
    "\n",
    "For each document $m \\in 1:M$, a topic, $z_{m} \\in 1:K$ is chosen according to the categorical distribution $\\boldsymbol{\\theta}$.  Where $\\boldsymbol{\\theta}$ is a $K$-dimension probability vector giving $p(\\theta_{k})$ for each topic $k \\in K$.  After the topic $z_{m}$ is chosen, the words of the document are generated independently conditional on that topic.  Specifically, word $n$ of document $m$ is chosen according to the categorical distribution $\\boldsymbol{\\phi_{z[m]}}$.  Here, $\\phi_{z[m]}$ gives the probability of each word of the vocabulary in documents belonging to topic $z_{m}$.\n",
    "\n",
    "Then in the general language given above, $p(topic)$ is given by $\\boldsymbol{\\theta}$ and $p(words|topic)$ is given by $\\boldsymbol{\\phi_{z[m]}}$.  Now, let's infer these values.  Note that Stan will use a 'fully' Bayesian approach, that is, not an emprical one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pystan\n",
    "import scipy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model specification\n",
    "\n",
    "topic_model = \"\"\"\n",
    "data {\n",
    "// training data\n",
    "int<lower=1> K; // num topics\n",
    "int<lower=1> V; // num words\n",
    "int<lower=0> M; // num docs\n",
    "int<lower=0> N; // total word instances\n",
    "int<lower=1,upper=K> z[M]; // topic for doc m\n",
    "int<lower=1,upper=V> w[N]; // word n\n",
    "int<lower=1,upper=M> doc[N]; // doc ID for word n\n",
    "// hyperparameters\n",
    "vector<lower=0>[K] alpha; // topic prior\n",
    "vector<lower=0>[V] beta; // word prior\n",
    "}\n",
    "\n",
    "parameters {\n",
    "simplex[K] theta; // topic prevalence\n",
    "simplex[V] phi[K]; // word dist for topic k\n",
    "}\n",
    "\n",
    "model {\n",
    "theta ~ dirichlet(alpha);\n",
    "for (k in 1:K)\n",
    "phi[k] ~ dirichlet(beta);\n",
    "for (m in 1:M)\n",
    "z[m] ~ categorical(theta);\n",
    "for (n in 1:N)\n",
    "w[n] ~ categorical(phi[z[doc[n]]]);\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simulated data\n",
    "\n",
    "# K is number of topics\n",
    "# V is the vocabulary\n",
    "# M is number of documents\n",
    "# N is total word instances\n",
    "# z[M] gives topic for doc m\n",
    "# w{N} gives word n\n",
    "# doc[N] gives doc ID for word n\n",
    "\n",
    "sim_data = {\n",
    "    \"K\":4, \"V\":10, \"M\": 200, \"N\": 1955,\n",
    "    \"z\": [1L, 1L, 2L, 1L, 1L, 3L, 2L, 3L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, \n",
    "2L, 4L, 4L, 1L, 1L, 1L, 1L, 4L, 1L, 2L, 1L, 1L, 1L, 3L, 1L, 3L, \n",
    "1L, 3L, 3L, 3L, 1L, 2L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 4L, 3L, 2L, \n",
    "1L, 2L, 3L, 1L, 1L, 1L, 1L, 2L, 2L, 1L, 2L, 1L, 1L, 1L, 1L, 1L, \n",
    "1L, 3L, 1L, 3L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, \n",
    "1L, 3L, 3L, 2L, 1L, 4L, 1L, 1L, 3L, 4L, 1L, 2L, 2L, 1L, 2L, 3L, \n",
    "1L, 2L, 3L, 4L, 1L, 2L, 3L, 3L, 3L, 1L, 2L, 4L, 3L, 3L, 4L, 1L, \n",
    "1L, 2L, 1L, 1L, 2L, 1L, 1L, 3L, 2L, 4L, 1L, 1L, 2L, 1L, 3L, 1L, \n",
    "1L, 2L, 2L, 4L, 2L, 2L, 1L, 1L, 2L, 4L, 1L, 2L, 2L, 2L, 1L, 2L, \n",
    "2L, 1L, 3L, 1L, 3L, 2L, 2L, 2L, 1L, 1L, 3L, 1L, 1L, 1L, 1L, 1L, \n",
    "1L, 4L, 1L, 1L, 1L, 3L, 2L, 4L, 1L, 1L, 1L, 3L, 1L, 2L, 1L, 1L, \n",
    "1L, 1L, 2L, 1L, 1L, 1L, 3L, 4L, 2L, 3L, 3L, 2L, 1L, 4L, 1L, 3L, \n",
    "1L, 2L, 1L, 1L, 1L, 1L, 3L, 4L, 1L],\n",
    "    \"w\": [7L, 9L, 5L, 2L, 9L, 1L, 1L, 9L, 6L, 9L, 1L, 10L, 6L, 7L, 2L, \n",
    "3L, 3L, 2L, 9L, 3L, 8L, 10L, 4L, 3L, 6L, 2L, 7L, 1L, 7L, 7L, \n",
    "1L, 6L, 7L, 7L, 1L, 7L, 7L, 4L, 7L, 7L, 4L, 1L, 7L, 6L, 6L, 3L, \n",
    "10L, 5L, 5L, 4L, 5L, 5L, 3L, 5L, 6L, 3L, 6L, 5L, 5L, 3L, 3L, \n",
    "3L, 7L, 9L, 2L, 2L, 9L, 8L, 5L, 8L, 9L, 1L, 7L, 1L, 7L, 7L, 1L, \n",
    "1L, 7L, 7L, 7L, 4L, 7L, 4L, 6L, 4L, 3L, 9L, 6L, 7L, 9L, 9L, 9L, \n",
    "9L, 6L, 3L, 7L, 1L, 1L, 7L, 1L, 4L, 7L, 8L, 1L, 9L, 9L, 6L, 8L, \n",
    "2L, 7L, 7L, 6L, 9L, 1L, 7L, 3L, 7L, 3L, 3L, 3L, 4L, 3L, 9L, 3L, \n",
    "3L, 3L, 1L, 4L, 9L, 10L, 1L, 6L, 7L, 6L, 1L, 7L, 7L, 1L, 3L, \n",
    "1L, 7L, 7L, 6L, 9L, 1L, 7L, 3L, 7L, 7L, 7L, 6L, 3L, 3L, 2L, 8L, \n",
    "3L, 2L, 3L, 3L, 1L, 2L, 3L, 2L, 2L, 1L, 3L, 2L, 9L, 9L, 2L, 10L, \n",
    "2L, 1L, 2L, 2L, 6L, 2L, 1L, 1L, 2L, 2L, 7L, 1L, 5L, 1L, 7L, 7L, \n",
    "7L, 8L, 3L, 5L, 7L, 9L, 7L, 9L, 7L, 7L, 1L, 6L, 1L, 10L, 7L, \n",
    "7L, 7L, 9L, 7L, 6L, 7L, 9L, 7L, 9L, 7L, 7L, 6L, 7L, 1L, 1L, 7L, \n",
    "7L, 1L, 7L, 2L, 2L, 1L, 4L, 2L, 5L, 7L, 2L, 1L, 9L, 4L, 1L, 7L, \n",
    "7L, 9L, 1L, 10L, 7L, 9L, 2L, 2L, 2L, 3L, 3L, 2L, 4L, 3L, 4L, \n",
    "4L, 4L, 4L, 7L, 1L, 6L, 7L, 1L, 7L, 7L, 7L, 1L, 7L, 9L, 4L, 3L, \n",
    "1L, 9L, 1L, 7L, 7L, 9L, 7L, 1L, 7L, 9L, 5L, 5L, 9L, 5L, 5L, 4L, \n",
    "8L, 1L, 9L, 7L, 6L, 7L, 7L, 7L, 9L, 6L, 8L, 5L, 5L, 2L, 9L, 5L, \n",
    "5L, 1L, 5L, 1L, 4L, 9L, 9L, 7L, 2L, 7L, 7L, 7L, 9L, 5L, 3L, 5L, \n",
    "5L, 5L, 6L, 5L, 6L, 1L, 5L, 6L, 3L, 6L, 5L, 4L, 6L, 4L, 3L, 5L, \n",
    "5L, 5L, 7L, 7L, 7L, 7L, 7L, 7L, 5L, 1L, 1L, 7L, 1L, 7L, 9L, 9L, \n",
    "2L, 4L, 2L, 3L, 3L, 4L, 3L, 1L, 1L, 7L, 1L, 4L, 7L, 8L, 7L, 6L, \n",
    "1L, 1L, 9L, 1L, 10L, 3L, 7L, 7L, 1L, 1L, 7L, 2L, 1L, 7L, 7L, \n",
    "9L, 4L, 3L, 3L, 2L, 3L, 3L, 2L, 9L, 3L, 9L, 7L, 9L, 7L, 4L, 1L, \n",
    "1L, 1L, 1L, 1L, 7L, 7L, 3L, 7L, 1L, 2L, 7L, 7L, 7L, 9L, 1L, 1L, \n",
    "7L, 1L, 9L, 7L, 1L, 7L, 9L, 7L, 7L, 6L, 7L, 1L, 2L, 2L, 2L, 1L, \n",
    "9L, 2L, 2L, 1L, 1L, 2L, 6L, 5L, 5L, 5L, 6L, 5L, 6L, 3L, 2L, 5L, \n",
    "5L, 3L, 5L, 5L, 3L, 3L, 8L, 9L, 3L, 3L, 3L, 9L, 4L, 3L, 10L, \n",
    "3L, 2L, 4L, 6L, 7L, 10L, 1L, 10L, 1L, 9L, 7L, 7L, 7L, 3L, 7L, \n",
    "4L, 9L, 4L, 4L, 4L, 3L, 3L, 3L, 6L, 3L, 2L, 2L, 6L, 7L, 6L, 5L, \n",
    "5L, 10L, 3L, 6L, 5L, 5L, 5L, 1L, 9L, 8L, 8L, 7L, 7L, 1L, 7L, \n",
    "7L, 9L, 1L, 3L, 1L, 5L, 10L, 7L, 7L, 6L, 7L, 7L, 7L, 7L, 7L, \n",
    "7L, 1L, 7L, 3L, 1L, 9L, 7L, 7L, 7L, 7L, 8L, 10L, 7L, 2L, 7L, \n",
    "5L, 7L, 4L, 7L, 7L, 9L, 5L, 7L, 6L, 9L, 1L, 6L, 4L, 3L, 2L, 3L, \n",
    "2L, 4L, 4L, 3L, 10L, 2L, 3L, 3L, 3L, 9L, 3L, 3L, 9L, 3L, 2L, \n",
    "4L, 3L, 3L, 6L, 9L, 3L, 3L, 7L, 7L, 1L, 6L, 9L, 3L, 7L, 9L, 3L, \n",
    "7L, 9L, 9L, 3L, 4L, 3L, 4L, 3L, 9L, 4L, 6L, 5L, 1L, 7L, 9L, 1L, \n",
    "7L, 4L, 1L, 7L, 7L, 7L, 6L, 7L, 7L, 7L, 9L, 7L, 1L, 6L, 9L, 7L, \n",
    "9L, 1L, 9L, 3L, 7L, 9L, 7L, 3L, 7L, 7L, 7L, 9L, 3L, 6L, 7L, 8L, \n",
    "7L, 7L, 7L, 7L, 4L, 3L, 7L, 7L, 9L, 7L, 1L, 1L, 1L, 7L, 1L, 7L, \n",
    "9L, 1L, 7L, 6L, 5L, 5L, 10L, 3L, 3L, 6L, 1L, 7L, 7L, 1L, 9L, \n",
    "7L, 1L, 3L, 9L, 7L, 7L, 5L, 5L, 6L, 3L, 5L, 5L, 1L, 5L, 5L, 5L, \n",
    "6L, 3L, 7L, 8L, 8L, 8L, 7L, 4L, 9L, 9L, 6L, 7L, 7L, 7L, 7L, 1L, \n",
    "2L, 7L, 1L, 4L, 9L, 7L, 7L, 6L, 1L, 7L, 7L, 7L, 7L, 7L, 7L, 9L, \n",
    "7L, 9L, 6L, 7L, 9L, 3L, 8L, 3L, 9L, 9L, 2L, 3L, 3L, 3L, 2L, 9L, \n",
    "3L, 7L, 10L, 4L, 7L, 1L, 7L, 1L, 9L, 7L, 7L, 7L, 4L, 7L, 1L, \n",
    "4L, 7L, 7L, 9L, 1L, 7L, 7L, 1L, 7L, 5L, 2L, 4L, 4L, 7L, 7L, 7L, \n",
    "7L, 7L, 9L, 3L, 4L, 4L, 1L, 3L, 3L, 4L, 3L, 3L, 9L, 9L, 3L, 2L, \n",
    "7L, 7L, 7L, 7L, 7L, 7L, 4L, 7L, 7L, 8L, 3L, 6L, 2L, 7L, 10L, \n",
    "3L, 2L, 7L, 8L, 3L, 7L, 1L, 7L, 1L, 7L, 8L, 9L, 1L, 7L, 7L, 7L, \n",
    "7L, 7L, 7L, 3L, 2L, 9L, 3L, 2L, 9L, 7L, 5L, 5L, 5L, 6L, 10L, \n",
    "8L, 5L, 9L, 6L, 1L, 5L, 1L, 5L, 7L, 7L, 3L, 2L, 4L, 3L, 7L, 4L, \n",
    "9L, 2L, 2L, 7L, 1L, 2L, 5L, 3L, 2L, 3L, 4L, 7L, 7L, 9L, 7L, 9L, \n",
    "1L, 1L, 6L, 9L, 5L, 9L, 9L, 2L, 6L, 2L, 10L, 2L, 9L, 1L, 1L, \n",
    "3L, 8L, 7L, 7L, 7L, 1L, 7L, 7L, 7L, 9L, 9L, 7L, 7L, 4L, 9L, 1L, \n",
    "4L, 8L, 5L, 5L, 5L, 5L, 6L, 2L, 1L, 3L, 5L, 1L, 2L, 4L, 7L, 2L, \n",
    "2L, 2L, 2L, 2L, 1L, 9L, 1L, 10L, 1L, 6L, 4L, 3L, 7L, 7L, 1L, \n",
    "7L, 3L, 2L, 6L, 4L, 3L, 2L, 8L, 3L, 3L, 2L, 8L, 2L, 7L, 6L, 3L, \n",
    "8L, 1L, 10L, 2L, 10L, 4L, 3L, 3L, 3L, 7L, 9L, 7L, 7L, 2L, 7L, \n",
    "4L, 3L, 2L, 3L, 5L, 3L, 3L, 9L, 9L, 8L, 5L, 6L, 8L, 3L, 5L, 5L, \n",
    "3L, 1L, 1L, 7L, 1L, 7L, 7L, 7L, 7L, 7L, 1L, 7L, 7L, 3L, 2L, 3L, \n",
    "3L, 2L, 3L, 2L, 2L, 6L, 5L, 5L, 5L, 1L, 2L, 2L, 5L, 5L, 10L, \n",
    "7L, 8L, 7L, 9L, 7L, 7L, 1L, 1L, 8L, 7L, 7L, 2L, 3L, 3L, 7L, 8L, \n",
    "9L, 5L, 6L, 3L, 3L, 5L, 3L, 5L, 5L, 5L, 5L, 5L, 3L, 1L, 6L, 3L, \n",
    "5L, 3L, 5L, 5L, 5L, 3L, 5L, 5L, 5L, 3L, 5L, 5L, 7L, 9L, 4L, 1L, \n",
    "7L, 1L, 7L, 3L, 3L, 5L, 3L, 6L, 3L, 3L, 3L, 2L, 3L, 3L, 3L, 2L, \n",
    "1L, 2L, 2L, 2L, 9L, 9L, 3L, 2L, 3L, 5L, 6L, 9L, 3L, 10L, 5L, \n",
    "5L, 5L, 5L, 6L, 6L, 5L, 2L, 2L, 2L, 3L, 2L, 1L, 2L, 2L, 1L, 5L, \n",
    "9L, 1L, 1L, 7L, 7L, 8L, 9L, 7L, 4L, 1L, 6L, 7L, 1L, 4L, 1L, 3L, \n",
    "3L, 3L, 2L, 2L, 7L, 3L, 2L, 3L, 2L, 2L, 4L, 10L, 2L, 2L, 3L, \n",
    "3L, 8L, 1L, 10L, 8L, 7L, 7L, 6L, 7L, 1L, 7L, 8L, 9L, 7L, 7L, \n",
    "7L, 7L, 7L, 1L, 1L, 1L, 1L, 3L, 7L, 7L, 1L, 7L, 7L, 4L, 3L, 3L, \n",
    "9L, 3L, 7L, 4L, 3L, 7L, 7L, 7L, 7L, 9L, 7L, 9L, 7L, 8L, 4L, 1L, \n",
    "1L, 3L, 10L, 3L, 5L, 3L, 5L, 6L, 6L, 1L, 3L, 3L, 2L, 3L, 2L, \n",
    "7L, 5L, 7L, 4L, 2L, 2L, 1L, 2L, 9L, 1L, 1L, 6L, 2L, 1L, 7L, 7L, \n",
    "3L, 10L, 1L, 7L, 7L, 3L, 1L, 1L, 1L, 2L, 7L, 6L, 1L, 9L, 7L, \n",
    "2L, 2L, 10L, 4L, 10L, 2L, 6L, 7L, 7L, 7L, 4L, 5L, 3L, 5L, 5L, \n",
    "5L, 6L, 8L, 6L, 7L, 7L, 3L, 4L, 7L, 7L, 9L, 4L, 7L, 10L, 6L, \n",
    "1L, 7L, 7L, 7L, 7L, 1L, 7L, 1L, 2L, 1L, 2L, 9L, 2L, 3L, 3L, 3L, \n",
    "10L, 3L, 3L, 3L, 9L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 2L, \n",
    "7L, 1L, 7L, 4L, 10L, 2L, 2L, 1L, 8L, 4L, 8L, 3L, 3L, 8L, 9L, \n",
    "3L, 3L, 4L, 3L, 3L, 3L, 2L, 6L, 2L, 3L, 4L, 3L, 3L, 2L, 7L, 7L, \n",
    "7L, 6L, 7L, 7L, 10L, 4L, 7L, 3L, 7L, 1L, 7L, 7L, 9L, 9L, 7L, \n",
    "1L, 7L, 4L, 9L, 1L, 9L, 7L, 7L, 3L, 2L, 3L, 9L, 3L, 9L, 3L, 2L, \n",
    "2L, 7L, 8L, 6L, 2L, 2L, 1L, 2L, 2L, 4L, 2L, 6L, 7L, 3L, 1L, 9L, \n",
    "4L, 7L, 3L, 3L, 1L, 3L, 7L, 3L, 3L, 9L, 2L, 3L, 10L, 3L, 10L, \n",
    "3L, 3L, 3L, 3L, 3L, 4L, 2L, 2L, 3L, 3L, 7L, 6L, 7L, 7L, 7L, 7L, \n",
    "7L, 3L, 3L, 5L, 6L, 5L, 9L, 4L, 7L, 9L, 3L, 3L, 9L, 3L, 2L, 9L, \n",
    "7L, 6L, 1L, 7L, 7L, 3L, 7L, 1L, 8L, 7L, 7L, 3L, 5L, 3L, 5L, 5L, \n",
    "6L, 3L, 2L, 7L, 7L, 6L, 7L, 7L, 1L, 7L, 6L, 7L, 1L, 5L, 5L, 5L, \n",
    "7L, 5L, 3L, 3L, 5L, 2L, 3L, 4L, 9L, 3L, 3L, 3L, 2L, 2L, 1L, 3L, \n",
    "2L, 4L, 2L, 3L, 2L, 9L, 3L, 4L, 3L, 3L, 7L, 4L, 3L, 7L, 7L, 2L, \n",
    "1L, 3L, 1L, 7L, 7L, 7L, 1L, 7L, 4L, 7L, 1L, 7L, 7L, 9L, 7L, 6L, \n",
    "7L, 5L, 7L, 1L, 7L, 8L, 6L, 6L, 3L, 5L, 6L, 3L, 5L, 5L, 5L, 5L, \n",
    "5L, 6L, 6L, 5L, 3L, 5L, 5L, 7L, 4L, 7L, 1L, 1L, 4L, 9L, 7L, 7L, \n",
    "7L, 7L, 7L, 2L, 7L, 7L, 1L, 1L, 7L, 1L, 3L, 1L, 6L, 3L, 2L, 9L, \n",
    "1L, 9L, 9L, 7L, 8L, 7L, 6L, 1L, 1L, 7L, 7L, 7L, 7L, 9L, 4L, 9L, \n",
    "7L, 1L, 6L, 8L, 1L, 2L, 2L, 1L, 9L, 7L, 2L, 1L, 2L, 9L, 9L, 6L, \n",
    "7L, 1L, 7L, 7L, 8L, 7L, 10L, 7L, 7L, 7L, 7L, 7L, 1L, 1L, 4L, \n",
    "1L, 7L, 7L, 1L, 1L, 1L, 1L, 7L, 7L, 4L, 7L, 5L, 5L, 6L, 5L, 10L, \n",
    "5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 5L, 2L, 3L, 10L, 3L, 3L, 4L, \n",
    "3L, 4L, 2L, 3L, 1L, 1L, 2L, 10L, 1L, 2L, 7L, 7L, 9L, 4L, 6L, \n",
    "7L, 7L, 5L, 1L, 8L, 7L, 6L, 1L, 9L, 1L, 3L, 2L, 3L, 8L, 1L, 7L, \n",
    "7L, 8L, 5L, 7L, 3L, 8L, 8L, 9L, 1L, 9L, 9L, 1L, 9L, 2L, 9L, 6L, \n",
    "5L, 3L, 6L, 6L, 9L, 5L, 4L, 9L, 3L, 5L, 5L, 1L, 5L, 5L, 6L, 7L, \n",
    "7L, 8L, 7L, 1L, 7L, 7L, 7L, 3L, 7L, 6L, 4L, 2L, 2L, 3L, 3L, 2L, \n",
    "3L, 3L, 4L, 3L, 3L, 3L, 3L, 3L, 4L, 7L, 6L, 7L, 9L, 7L, 1L, 7L, \n",
    "4L, 7L, 7L, 7L, 9L, 1L, 4L, 9L, 7L, 7L, 7L, 1L, 7L, 7L, 7L, 1L, \n",
    "6L, 7L, 7L, 1L, 1L, 3L, 7L, 7L, 7L, 6L, 7L, 3L, 5L, 9L, 7L, 7L, \n",
    "7L, 1L, 9L, 7L, 7L, 1L, 3L, 3L, 1L, 3L, 3L, 3L, 4L, 2L, 2L, 3L, \n",
    "3L, 3L, 6L, 1L, 3L, 7L, 3L, 4L, 7L, 7L, 9L, 4L, 6L, 10L, 9L, \n",
    "7L, 6L, 7L, 1L, 2L, 1L, 1L, 7L, 1L, 1L, 7L, 7L, 7L, 7L, 7L, 9L, \n",
    "1L, 7L, 7L, 7L, 9L, 9L, 8L, 10L, 5L, 4L, 5L, 5L, 3L, 5L, 3L, \n",
    "1L, 6L, 8L, 5L, 1L, 4L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 6L, 2L, 4L, \n",
    "9L, 3L, 3L, 9L, 10L, 3L, 3L, 9L, 6L, 6L, 3L, 8L, 5L, 3L, 5L, \n",
    "3L, 1L, 7L, 4L, 2L, 6L, 3L, 10L, 7L, 8L, 9L, 7L, 7L, 7L, 7L, \n",
    "7L, 2L, 1L, 2L, 2L, 2L, 1L, 5L, 7L, 2L, 2L, 1L, 2L, 9L, 4L, 1L, \n",
    "7L, 7L, 2L, 4L, 7L, 6L, 6L, 10L, 5L, 6L, 5L, 5L, 8L, 9L, 5L, \n",
    "5L, 7L, 1L, 6L, 9L, 7L, 7L, 7L, 3L, 3L, 7L, 3L, 2L, 1L, 3L, 4L, \n",
    "7L, 1L, 7L, 6L, 7L, 7L, 7L, 7L, 4L, 7L, 5L, 9L, 6L, 9L, 4L, 7L, \n",
    "1L, 7L, 7L, 1L, 1L, 7L, 7L, 1L, 7L, 7L, 6L, 7L, 7L, 1L, 7L, 8L, \n",
    "7L, 7L, 9L, 7L, 9L, 7L, 2L, 4L, 9L, 9L, 5L, 5L, 4L, 5L, 1L, 3L, \n",
    "3L, 7L, 5L, 2L, 1L, 7L, 5L, 5L, 9L, 9L, 2L, 2L, 2L, 5L, 5L, 1L, \n",
    "7L, 1L, 7L, 2L, 4L, 7L, 7L, 9L, 1L, 7L, 9L], \n",
    "    \"doc\": [1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, \n",
    "3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, \n",
    "4L, 4L, 4L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, \n",
    "6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, \n",
    "7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, \n",
    "9L, 9L, 9L, 9L, 9L, 9L, 9L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, \n",
    "10L, 10L, 10L, 10L, 10L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, \n",
    "11L, 11L, 11L, 11L, 11L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 13L, \n",
    "13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 14L, 14L, 14L, \n",
    "14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 15L, 15L, 15L, \n",
    "15L, 15L, 15L, 15L, 15L, 15L, 15L, 16L, 16L, 16L, 16L, 16L, 16L, \n",
    "16L, 16L, 16L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, \n",
    "17L, 17L, 17L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 19L, 19L, 19L, \n",
    "19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 20L, 20L, 20L, 20L, 20L, \n",
    "20L, 20L, 20L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, \n",
    "21L, 21L, 21L, 21L, 21L, 21L, 21L, 22L, 22L, 22L, 23L, 23L, 23L, \n",
    "23L, 23L, 23L, 23L, 23L, 23L, 23L, 23L, 23L, 23L, 24L, 24L, 24L, \n",
    "24L, 24L, 24L, 24L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, \n",
    "25L, 25L, 25L, 25L, 26L, 26L, 26L, 26L, 26L, 27L, 27L, 27L, 27L, \n",
    "27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 28L, 28L, 28L, 28L, 28L, \n",
    "28L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 30L, 30L, 30L, 30L, 30L, \n",
    "30L, 30L, 31L, 31L, 31L, 31L, 31L, 31L, 31L, 31L, 31L, 31L, 31L, \n",
    "32L, 32L, 32L, 32L, 32L, 32L, 32L, 32L, 32L, 32L, 33L, 33L, 33L, \n",
    "33L, 33L, 34L, 34L, 34L, 34L, 34L, 34L, 34L, 34L, 34L, 35L, 35L, \n",
    "35L, 35L, 35L, 35L, 35L, 36L, 36L, 36L, 36L, 36L, 36L, 36L, 36L, \n",
    "36L, 36L, 36L, 36L, 36L, 36L, 37L, 37L, 37L, 37L, 37L, 37L, 37L, \n",
    "38L, 38L, 38L, 38L, 38L, 38L, 38L, 38L, 38L, 38L, 38L, 39L, 39L, \n",
    "39L, 39L, 39L, 39L, 39L, 39L, 40L, 40L, 40L, 40L, 40L, 40L, 40L, \n",
    "41L, 41L, 41L, 41L, 41L, 41L, 41L, 41L, 42L, 42L, 42L, 42L, 42L, \n",
    "42L, 42L, 42L, 42L, 43L, 43L, 43L, 43L, 43L, 43L, 43L, 43L, 43L, \n",
    "43L, 43L, 43L, 43L, 43L, 43L, 43L, 44L, 44L, 44L, 44L, 44L, 44L, \n",
    "44L, 44L, 44L, 45L, 45L, 45L, 45L, 45L, 45L, 45L, 45L, 45L, 45L, \n",
    "46L, 46L, 46L, 46L, 46L, 46L, 46L, 46L, 46L, 46L, 46L, 46L, 46L, \n",
    "46L, 46L, 47L, 47L, 47L, 47L, 47L, 47L, 47L, 47L, 47L, 47L, 47L, \n",
    "47L, 47L, 48L, 48L, 48L, 48L, 48L, 48L, 48L, 48L, 48L, 48L, 48L, \n",
    "48L, 48L, 48L, 49L, 49L, 49L, 49L, 49L, 49L, 49L, 49L, 49L, 49L, \n",
    "50L, 50L, 50L, 50L, 50L, 50L, 50L, 50L, 50L, 50L, 50L, 51L, 51L, \n",
    "51L, 51L, 51L, 51L, 51L, 51L, 51L, 51L, 51L, 51L, 52L, 52L, 52L, \n",
    "52L, 52L, 52L, 52L, 52L, 52L, 52L, 52L, 52L, 52L, 52L, 52L, 53L, \n",
    "53L, 53L, 53L, 53L, 53L, 53L, 53L, 53L, 53L, 53L, 53L, 53L, 53L, \n",
    "54L, 54L, 54L, 54L, 54L, 54L, 54L, 54L, 54L, 55L, 55L, 55L, 55L, \n",
    "55L, 55L, 55L, 55L, 55L, 55L, 55L, 55L, 55L, 55L, 56L, 56L, 56L, \n",
    "56L, 56L, 56L, 56L, 56L, 56L, 56L, 56L, 57L, 57L, 57L, 57L, 57L, \n",
    "57L, 57L, 57L, 57L, 58L, 58L, 58L, 58L, 58L, 58L, 58L, 58L, 58L, \n",
    "58L, 58L, 59L, 59L, 59L, 59L, 59L, 59L, 59L, 59L, 59L, 59L, 59L, \n",
    "60L, 60L, 60L, 60L, 60L, 60L, 60L, 60L, 60L, 60L, 60L, 60L, 61L, \n",
    "61L, 61L, 61L, 61L, 61L, 61L, 61L, 61L, 62L, 62L, 62L, 62L, 62L, \n",
    "62L, 62L, 62L, 62L, 62L, 63L, 63L, 63L, 63L, 63L, 63L, 63L, 64L, \n",
    "64L, 64L, 64L, 64L, 64L, 64L, 64L, 65L, 65L, 65L, 65L, 65L, 65L, \n",
    "65L, 65L, 66L, 66L, 66L, 66L, 66L, 66L, 66L, 66L, 66L, 66L, 67L, \n",
    "67L, 67L, 67L, 67L, 67L, 67L, 67L, 67L, 67L, 67L, 67L, 67L, 67L, \n",
    "67L, 67L, 67L, 68L, 68L, 68L, 68L, 68L, 68L, 68L, 68L, 69L, 69L, \n",
    "69L, 69L, 69L, 69L, 69L, 69L, 69L, 69L, 69L, 70L, 70L, 70L, 70L, \n",
    "70L, 70L, 70L, 70L, 70L, 70L, 71L, 71L, 71L, 71L, 71L, 71L, 71L, \n",
    "71L, 71L, 71L, 71L, 71L, 71L, 72L, 72L, 72L, 72L, 72L, 72L, 72L, \n",
    "72L, 72L, 72L, 73L, 73L, 73L, 73L, 73L, 73L, 73L, 73L, 73L, 73L, \n",
    "73L, 73L, 73L, 74L, 74L, 74L, 74L, 74L, 74L, 74L, 74L, 74L, 75L, \n",
    "75L, 75L, 75L, 75L, 75L, 75L, 75L, 75L, 75L, 76L, 76L, 76L, 76L, \n",
    "76L, 76L, 76L, 76L, 76L, 76L, 76L, 76L, 76L, 76L, 77L, 77L, 77L, \n",
    "77L, 77L, 77L, 77L, 77L, 77L, 77L, 77L, 78L, 78L, 78L, 78L, 78L, \n",
    "78L, 78L, 78L, 78L, 78L, 78L, 78L, 79L, 79L, 79L, 79L, 79L, 79L, \n",
    "80L, 80L, 81L, 81L, 81L, 82L, 82L, 82L, 82L, 82L, 82L, 82L, 82L, \n",
    "82L, 82L, 82L, 83L, 83L, 83L, 83L, 83L, 83L, 83L, 83L, 83L, 83L, \n",
    "83L, 83L, 83L, 83L, 83L, 83L, 83L, 83L, 84L, 84L, 84L, 84L, 84L, \n",
    "84L, 84L, 84L, 84L, 85L, 85L, 85L, 85L, 85L, 85L, 85L, 85L, 85L, \n",
    "85L, 86L, 86L, 86L, 86L, 86L, 86L, 86L, 87L, 87L, 87L, 87L, 87L, \n",
    "87L, 87L, 87L, 87L, 87L, 87L, 88L, 88L, 88L, 88L, 88L, 88L, 88L, \n",
    "88L, 88L, 88L, 89L, 89L, 89L, 89L, 89L, 89L, 89L, 89L, 89L, 89L, \n",
    "90L, 90L, 90L, 90L, 90L, 90L, 90L, 90L, 90L, 90L, 90L, 91L, 91L, \n",
    "91L, 91L, 91L, 91L, 91L, 91L, 92L, 92L, 92L, 92L, 92L, 92L, 92L, \n",
    "92L, 92L, 92L, 92L, 92L, 92L, 92L, 92L, 92L, 93L, 93L, 93L, 93L, \n",
    "93L, 93L, 94L, 94L, 94L, 94L, 94L, 94L, 94L, 94L, 94L, 94L, 95L, \n",
    "95L, 95L, 95L, 95L, 95L, 96L, 96L, 96L, 96L, 96L, 96L, 96L, 96L, \n",
    "96L, 96L, 96L, 96L, 96L, 97L, 97L, 97L, 97L, 97L, 97L, 97L, 97L, \n",
    "98L, 98L, 98L, 98L, 98L, 99L, 99L, 99L, 99L, 100L, 100L, 100L, \n",
    "100L, 100L, 100L, 100L, 100L, 100L, 100L, 100L, 100L, 101L, 101L, \n",
    "101L, 101L, 102L, 102L, 102L, 102L, 102L, 102L, 102L, 102L, 102L, \n",
    "102L, 103L, 103L, 103L, 103L, 103L, 103L, 103L, 103L, 103L, 103L, \n",
    "104L, 104L, 104L, 104L, 104L, 104L, 104L, 104L, 104L, 105L, 105L, \n",
    "105L, 105L, 105L, 105L, 105L, 106L, 106L, 106L, 106L, 106L, 106L, \n",
    "106L, 106L, 106L, 106L, 106L, 106L, 107L, 107L, 107L, 107L, 107L, \n",
    "107L, 107L, 107L, 107L, 108L, 108L, 108L, 108L, 108L, 109L, 109L, \n",
    "109L, 109L, 109L, 109L, 109L, 109L, 110L, 110L, 110L, 110L, 110L, \n",
    "110L, 110L, 110L, 110L, 110L, 110L, 111L, 111L, 111L, 111L, 111L, \n",
    "112L, 112L, 112L, 112L, 112L, 112L, 112L, 112L, 112L, 113L, 113L, \n",
    "113L, 113L, 113L, 113L, 113L, 113L, 113L, 113L, 113L, 113L, 113L, \n",
    "113L, 113L, 113L, 113L, 113L, 114L, 114L, 114L, 114L, 114L, 114L, \n",
    "114L, 114L, 114L, 114L, 114L, 114L, 115L, 115L, 115L, 115L, 115L, \n",
    "115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 116L, 116L, \n",
    "116L, 116L, 116L, 117L, 117L, 117L, 117L, 117L, 117L, 118L, 118L, \n",
    "118L, 118L, 118L, 118L, 118L, 118L, 118L, 118L, 119L, 119L, 119L, \n",
    "119L, 119L, 119L, 119L, 119L, 120L, 120L, 120L, 120L, 120L, 120L, \n",
    "120L, 120L, 121L, 121L, 121L, 121L, 121L, 121L, 121L, 121L, 121L, \n",
    "121L, 122L, 122L, 122L, 122L, 122L, 122L, 122L, 122L, 123L, 123L, \n",
    "123L, 123L, 123L, 123L, 123L, 123L, 123L, 123L, 124L, 124L, 124L, \n",
    "124L, 124L, 124L, 125L, 125L, 125L, 125L, 126L, 126L, 126L, 126L, \n",
    "126L, 126L, 126L, 126L, 127L, 127L, 127L, 127L, 127L, 127L, 127L, \n",
    "127L, 127L, 127L, 127L, 127L, 128L, 128L, 128L, 128L, 128L, 128L, \n",
    "128L, 128L, 128L, 128L, 129L, 129L, 129L, 129L, 129L, 129L, 129L, \n",
    "129L, 129L, 129L, 129L, 130L, 130L, 130L, 130L, 130L, 130L, 130L, \n",
    "130L, 130L, 130L, 131L, 131L, 131L, 131L, 131L, 131L, 131L, 131L, \n",
    "132L, 132L, 132L, 132L, 132L, 132L, 132L, 133L, 133L, 133L, 133L, \n",
    "133L, 133L, 133L, 133L, 133L, 133L, 133L, 133L, 133L, 133L, 134L, \n",
    "134L, 134L, 134L, 134L, 134L, 134L, 134L, 134L, 134L, 134L, 134L, \n",
    "134L, 134L, 134L, 135L, 135L, 135L, 135L, 135L, 135L, 135L, 135L, \n",
    "135L, 135L, 136L, 136L, 136L, 136L, 136L, 136L, 136L, 137L, 137L, \n",
    "137L, 137L, 137L, 137L, 137L, 137L, 137L, 137L, 137L, 137L, 138L, \n",
    "138L, 138L, 138L, 138L, 138L, 138L, 139L, 139L, 139L, 139L, 139L, \n",
    "139L, 140L, 140L, 140L, 140L, 140L, 140L, 140L, 140L, 141L, 141L, \n",
    "141L, 141L, 141L, 141L, 141L, 141L, 141L, 141L, 142L, 142L, 142L, \n",
    "142L, 142L, 142L, 143L, 143L, 143L, 143L, 144L, 144L, 144L, 144L, \n",
    "144L, 144L, 144L, 144L, 144L, 144L, 145L, 145L, 145L, 145L, 145L, \n",
    "145L, 145L, 145L, 145L, 145L, 145L, 145L, 146L, 146L, 146L, 146L, \n",
    "146L, 146L, 146L, 146L, 147L, 147L, 147L, 147L, 147L, 147L, 147L, \n",
    "147L, 147L, 147L, 148L, 148L, 148L, 148L, 148L, 148L, 148L, 148L, \n",
    "148L, 149L, 149L, 149L, 149L, 149L, 149L, 150L, 150L, 150L, 150L, \n",
    "150L, 150L, 150L, 150L, 150L, 150L, 150L, 150L, 150L, 150L, 151L, \n",
    "151L, 151L, 151L, 151L, 151L, 152L, 152L, 152L, 152L, 152L, 152L, \n",
    "152L, 152L, 152L, 152L, 153L, 153L, 153L, 153L, 153L, 153L, 153L, \n",
    "153L, 153L, 153L, 153L, 154L, 154L, 154L, 154L, 154L, 154L, 154L, \n",
    "154L, 154L, 154L, 154L, 154L, 154L, 154L, 154L, 154L, 154L, 155L, \n",
    "155L, 155L, 155L, 155L, 155L, 155L, 155L, 156L, 156L, 157L, 157L, \n",
    "157L, 157L, 157L, 158L, 158L, 158L, 158L, 158L, 158L, 158L, 159L, \n",
    "159L, 159L, 159L, 159L, 159L, 159L, 159L, 159L, 159L, 159L, 159L, \n",
    "159L, 160L, 160L, 160L, 160L, 160L, 160L, 160L, 160L, 160L, 160L, \n",
    "160L, 160L, 161L, 161L, 161L, 161L, 161L, 161L, 161L, 161L, 161L, \n",
    "162L, 162L, 162L, 162L, 163L, 163L, 163L, 163L, 163L, 163L, 163L, \n",
    "163L, 163L, 164L, 164L, 164L, 164L, 164L, 164L, 164L, 164L, 164L, \n",
    "164L, 164L, 164L, 164L, 164L, 165L, 165L, 165L, 165L, 165L, 165L, \n",
    "165L, 165L, 165L, 165L, 165L, 165L, 165L, 165L, 165L, 165L, 166L, \n",
    "166L, 166L, 166L, 166L, 166L, 166L, 166L, 166L, 167L, 167L, 167L, \n",
    "167L, 167L, 167L, 168L, 168L, 168L, 168L, 168L, 168L, 168L, 168L, \n",
    "168L, 168L, 168L, 168L, 168L, 169L, 169L, 169L, 169L, 169L, 169L, \n",
    "169L, 169L, 169L, 170L, 170L, 170L, 170L, 170L, 170L, 170L, 170L, \n",
    "170L, 170L, 170L, 170L, 170L, 171L, 171L, 171L, 171L, 171L, 171L, \n",
    "171L, 171L, 171L, 171L, 171L, 171L, 171L, 171L, 171L, 171L, 171L, \n",
    "172L, 172L, 172L, 172L, 172L, 172L, 172L, 172L, 172L, 172L, 172L, \n",
    "173L, 173L, 173L, 173L, 173L, 173L, 173L, 173L, 173L, 173L, 173L, \n",
    "173L, 173L, 173L, 174L, 174L, 174L, 174L, 174L, 174L, 174L, 174L, \n",
    "174L, 174L, 174L, 174L, 174L, 174L, 174L, 174L, 175L, 175L, 175L, \n",
    "175L, 175L, 175L, 175L, 175L, 175L, 175L, 175L, 175L, 176L, 176L, \n",
    "176L, 176L, 176L, 176L, 176L, 176L, 176L, 176L, 177L, 177L, 177L, \n",
    "177L, 177L, 177L, 177L, 177L, 177L, 178L, 178L, 178L, 178L, 178L, \n",
    "178L, 178L, 178L, 178L, 178L, 178L, 178L, 179L, 179L, 179L, 179L, \n",
    "179L, 179L, 179L, 179L, 179L, 179L, 180L, 180L, 180L, 180L, 180L, \n",
    "180L, 180L, 180L, 181L, 181L, 181L, 181L, 181L, 181L, 181L, 181L, \n",
    "181L, 181L, 181L, 181L, 181L, 181L, 181L, 181L, 182L, 182L, 182L, \n",
    "182L, 182L, 182L, 182L, 182L, 182L, 182L, 182L, 182L, 182L, 183L, \n",
    "183L, 183L, 183L, 183L, 183L, 183L, 183L, 183L, 183L, 183L, 184L, \n",
    "184L, 184L, 184L, 184L, 184L, 184L, 184L, 184L, 185L, 185L, 186L, \n",
    "186L, 186L, 186L, 186L, 187L, 187L, 187L, 187L, 187L, 187L, 187L, \n",
    "188L, 188L, 188L, 188L, 188L, 188L, 188L, 188L, 188L, 189L, 189L, \n",
    "189L, 189L, 189L, 189L, 189L, 189L, 189L, 189L, 189L, 189L, 190L, \n",
    "190L, 190L, 190L, 190L, 190L, 190L, 190L, 190L, 190L, 191L, 191L, \n",
    "191L, 191L, 191L, 191L, 191L, 191L, 191L, 192L, 192L, 192L, 192L, \n",
    "192L, 192L, 192L, 193L, 193L, 193L, 193L, 193L, 193L, 193L, 193L, \n",
    "194L, 194L, 194L, 194L, 194L, 194L, 194L, 195L, 195L, 195L, 195L, \n",
    "195L, 195L, 195L, 195L, 195L, 195L, 195L, 195L, 196L, 196L, 196L, \n",
    "196L, 196L, 196L, 196L, 196L, 196L, 196L, 196L, 196L, 197L, 197L, \n",
    "197L, 197L, 197L, 197L, 197L, 197L, 197L, 197L, 197L, 198L, 198L, \n",
    "198L, 198L, 198L, 198L, 198L, 198L, 198L, 198L, 198L, 198L, 198L, \n",
    "198L, 199L, 199L, 199L, 199L, 199L, 199L, 199L, 200L, 200L, 200L, \n",
    "200L, 200L, 200L, 200L, 200L, 200L, 200L, 200L, 200L], \n",
    "    \"alpha\": [1, 1, 1, 1], \"beta\": [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit model\n",
    "fit = pystan.stan(model_code=topic_model, data=sim_data, iter=1000, chains=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for Stan model: anon_model_20e980a24ef3b0be928a8c38650f65e3.\n",
      "4 chains, each with iter=1000; warmup=500; thin=1; \n",
      "post-warmup draws per chain=500, total post-warmup draws=2000.\n",
      "\n",
      "           mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "theta[0]   0.52  1.3e-3   0.03   0.45   0.49   0.51   0.54   0.58  668.0    1.0\n",
      "theta[1]   0.23  1.1e-3   0.03   0.18   0.21   0.23   0.25   0.28  668.0    1.0\n",
      "theta[2]   0.17  1.1e-3   0.03   0.12   0.15   0.17   0.19   0.22  668.0    1.0\n",
      "theta[3]   0.09  7.3e-4   0.02   0.05   0.07   0.09    0.1   0.13  668.0    1.0\n",
      "phi[0,0]   0.19  4.4e-4   0.01   0.17   0.18   0.19    0.2   0.21  668.0    1.0\n",
      "phi[1,0]   0.02  2.7e-4 7.0e-3   0.01   0.02   0.02   0.03   0.04  668.0    1.0\n",
      "phi[2,0]   0.05  4.6e-4   0.01   0.02   0.04   0.04   0.05   0.07  668.0    1.0\n",
      "phi[3,0]    0.2  1.2e-3   0.03   0.14   0.18    0.2   0.22   0.27  668.0    1.0\n",
      "phi[0,1]   0.02  1.8e-4 4.5e-3   0.01   0.02   0.02   0.02   0.03  668.0    1.0\n",
      "phi[1,1]   0.18  7.2e-4   0.02   0.15   0.17   0.18   0.19   0.22  668.0    1.0\n",
      "phi[2,1]   0.02  3.1e-4 8.1e-3 9.3e-3   0.02   0.02   0.03   0.04  668.0    1.0\n",
      "phi[3,1]   0.47  1.5e-3   0.04    0.4   0.45   0.47    0.5   0.55  668.0    1.0\n",
      "phi[0,2]   0.04  2.3e-4 6.1e-3   0.03   0.04   0.04   0.05   0.06  668.0    1.0\n",
      "phi[1,2]   0.45  8.9e-4   0.02   0.41   0.44   0.45   0.47    0.5  668.0    1.0\n",
      "phi[2,2]   0.14  8.1e-4   0.02   0.11   0.13   0.14   0.16   0.19  668.0    1.0\n",
      "phi[3,2]   0.02  4.5e-4   0.01 7.5e-3   0.02   0.02   0.03   0.05  668.0    1.0\n",
      "phi[0,3]   0.05  2.6e-4 6.8e-3   0.04   0.05   0.05   0.06   0.07  668.0    1.0\n",
      "phi[1,3]   0.12  5.8e-4   0.01   0.09   0.11   0.11   0.12   0.15  668.0    1.0\n",
      "phi[2,3]   0.03  3.5e-4 9.2e-3   0.01   0.02   0.02   0.03   0.05  668.0    1.0\n",
      "phi[3,3]   0.04  6.1e-4   0.02   0.02   0.03   0.04   0.05   0.08  668.0    1.0\n",
      "phi[0,4]   0.01  1.4e-4 3.7e-3 8.4e-3   0.01   0.01   0.02   0.02  668.0    1.0\n",
      "phi[1,4]   0.01  2.1e-4 5.4e-3 5.1e-3 9.2e-3   0.01   0.02   0.03  668.0    1.0\n",
      "phi[2,4]   0.46  1.1e-3   0.03   0.41   0.44   0.46   0.48   0.51  668.0    1.0\n",
      "phi[3,4]   0.05  6.5e-4   0.02   0.02   0.04   0.05   0.06   0.09  668.0    1.0\n",
      "phi[0,5]   0.06  2.9e-4 7.5e-3   0.05   0.06   0.06   0.07   0.08  668.0    1.0\n",
      "phi[1,5]   0.02  2.7e-4 7.0e-3   0.01   0.02   0.02   0.03   0.04  668.0    1.0\n",
      "phi[2,5]   0.16  8.4e-4   0.02   0.12   0.14   0.16   0.17    0.2  668.0    1.0\n",
      "phi[3,5]   0.03  5.3e-4   0.01 9.9e-3   0.02   0.03   0.04   0.07  668.0    1.0\n",
      "phi[0,6]   0.45  6.0e-4   0.02   0.42   0.43   0.44   0.46   0.48  668.0    1.0\n",
      "phi[1,6]   0.05  4.0e-4   0.01   0.03   0.04   0.05   0.06   0.07  668.0    1.0\n",
      "phi[2,6]   0.03  3.4e-4 8.9e-3   0.01   0.02   0.02   0.03   0.05  668.0    1.0\n",
      "phi[3,6]   0.05  6.9e-4   0.02   0.02   0.04   0.05   0.06   0.09  668.0    1.0\n",
      "phi[0,7]   0.03  2.0e-4 5.2e-3   0.02   0.02   0.03   0.03   0.04  668.0    1.0\n",
      "phi[1,7]   0.03  3.0e-4 7.9e-3   0.02   0.02   0.03   0.03   0.05  668.0    1.0\n",
      "phi[2,7]   0.05  5.0e-4   0.01   0.03   0.05   0.05   0.06   0.08  668.0    1.0\n",
      "phi[3,7] 6.8e-3  2.5e-4 6.6e-3 2.0e-4 2.1e-3 4.9e-3 9.4e-3   0.02  668.0    1.0\n",
      "phi[0,8]   0.12  4.0e-4   0.01    0.1   0.11   0.12   0.13   0.14  668.0    1.0\n",
      "phi[1,8]   0.09  5.3e-4   0.01   0.06   0.08   0.08   0.09   0.11  668.0    1.0\n",
      "phi[2,8]   0.04  4.2e-4   0.01   0.02   0.03   0.03   0.04   0.06  668.0    1.0\n",
      "phi[3,8]   0.09  9.0e-4   0.02   0.05   0.08   0.09   0.11   0.14  668.0    1.0\n",
      "phi[0,9]   0.02  1.7e-4 4.5e-3   0.01   0.02   0.02   0.02   0.03  668.0    1.0\n",
      "phi[1,9]   0.03  3.3e-4 8.5e-3   0.02   0.03   0.03   0.04   0.05  668.0    1.0\n",
      "phi[2,9]   0.03  3.6e-4 9.4e-3   0.01   0.02   0.03   0.03   0.05  668.0    1.0\n",
      "phi[3,9]   0.02  4.8e-4   0.01 7.0e-3   0.02   0.02   0.03   0.05  668.0    1.0\n",
      "lp__      -3617    0.23   4.46  -3627  -3619  -3616  -3613  -3609  386.0    1.0\n",
      "\n",
      "Samples were drawn using NUTS(diag_e) at Mon Mar 14 21:34:57 2016.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n"
     ]
    }
   ],
   "source": [
    "print fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Naive Bayes classifier function\n",
    "# remember that we only need to maximize the numerator\n",
    "\n",
    "def NaiveBayesClassifier(doc):\n",
    "    posteriors = {}\n",
    "    \n",
    "    for theta in range(0,4):\n",
    "        theta_prob = np.mean(fit.get_posterior_mean()[theta])\n",
    "        likelihood = 1\n",
    "        for word in doc:\n",
    "            index = 4*(word+1)+theta\n",
    "            conditional_prob = np.mean(fit.get_posterior_mean()[index])\n",
    "            likelihood = likelihood*conditional_prob\n",
    "        posterior = theta_prob*likelihood\n",
    "        posteriors[theta] = posterior\n",
    "        \n",
    "    import operator\n",
    "    topic_classify = max(posteriors.iteritems(), key=operator.itemgetter(1))[0]\n",
    "    results = {}\n",
    "    results['classification'] =  \"document belongs to topic \"+str(topic_classify)\n",
    "    results['posteriors'] = posteriors\n",
    "    return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make up a new document\n",
    "new_doc = [1,2,3,4,9,9,9,9,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run classifier\n",
    "classification =  NaiveBayesClassifier(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 7.8369562115686165e-16, 1: 8.3871795065309505e-13, 2: 1.335157820849888e-13, 3: 2.1418205728046378e-14}\n",
      "document belongs to topic 1\n"
     ]
    }
   ],
   "source": [
    "# printn results\n",
    "print classification['posteriors']\n",
    "print classification['classification']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "This is a very simple example but you can imagine this type of problem over a much larger dataset or this type of inference needed in more complex NLP settings.\n",
    "\n",
    "##### Sources\n",
    "[Stan wikipedia page](https://en.wikipedia.org/wiki/Stan_(software)\n",
    "\n",
    "\n",
    "[Official Stan webpage](http://mc-stan.org/)\n",
    "\n",
    "\n",
    "[Stan reference manual](https://github.com/stan-dev/stan/releases/download/v2.9.0/stan-reference-2.9.0.pdf)\n",
    "\n",
    "\n",
    "[Blog post about Stan](http://ouzor.github.io/blog/2016/02/09/probabilistic-programming.html)\n",
    "\n",
    "\n",
    "[Stan examples github](https://github.com/stan-dev/example-models/wiki)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
